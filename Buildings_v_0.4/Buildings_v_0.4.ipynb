{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T07:40:10.510850Z",
     "iopub.status.busy": "2023-03-27T07:40:10.510375Z",
     "iopub.status.idle": "2023-03-27T07:40:12.528437Z",
     "shell.execute_reply": "2023-03-27T07:40:12.527481Z",
     "shell.execute_reply.started": "2023-03-27T07:40:10.510804Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import datetime\n",
    "from functools import partial\n",
    "from glob import glob\n",
    "import math\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "import random\n",
    "from typing import Any, Dict, Optional\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.amp import autocast # allows for differentr datatypes (torch.32/ torch.16)\n",
    "from torchvision import transforms as T\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from cjm_pytorch_utils.core import pil_to_tensor, tensor_to_pil, get_torch_device, set_seed, denorm_img_tensor, move_data_to_device\n",
    "from cjm_psl_utils.core import download_file, file_extract, get_source_code\n",
    "from cjm_pandas_utils.core import markdown_to_pandas, convert_to_numeric, convert_to_string\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtnt.utils import get_module_summary\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import imageio.v3 as iio\n",
    "import ipympl\n",
    "import skimage as ski\n",
    "\n",
    "# Import Mask R-CNN\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn_v2, MaskRCNN, maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection import MaskRCNN_ResNet50_FPN_V2_Weights, MaskRCNN_ResNet50_FPN_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_torch_device()\n",
    "dtype = torch.float32\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up projects's directory\n",
    "# project's name\n",
    "project_name = f\"pytorch-buildings-maskrcnn\"\n",
    "\n",
    "# path for project's folder\n",
    "project_dir = Path(f\"/{project_name}/\")\n",
    "\n",
    "# create the project directory\n",
    "project_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T07:40:12.994007Z",
     "iopub.status.busy": "2023-03-27T07:40:12.993495Z",
     "iopub.status.idle": "2023-03-27T07:40:13.162589Z",
     "shell.execute_reply": "2023-03-27T07:40:13.161654Z",
     "shell.execute_reply.started": "2023-03-27T07:40:12.993973Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load images\n",
    "\n",
    "images_dir = \"C:/Users/kubaw/Desktop/DELFT/CORE/satellite_predictions/Dataset2/Images/Destroyed\"  \n",
    "image_filenames = sorted(os.listdir(images_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply transform to the images (normalise)\n",
    "def load_images_from_folder(images_dir):\n",
    "    images = []\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "    for filename in os.listdir(images_dir):\n",
    "        if filename.endswith('.jpeg'):\n",
    "            img = Image.open(os.path.join(images_dir, filename))\n",
    "            img_t = transform(img)\n",
    "            images.append(img_t)\n",
    "    return images\n",
    "\n",
    "image_tensors = load_images_from_folder(images_dir)\n",
    "\n",
    "len(image_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Make image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_index_list(image_tensors):\n",
    "    num_images = len(image_tensors)\n",
    "    return list(range(num_images))\n",
    "\n",
    "image_ids = generate_index_list(image_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Load Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_dir = \"C:/Users/kubaw/Desktop/DELFT/CORE/satellite_predictions/Dataset2/Masks/Destroyed\"  \n",
    "mask_filenames = sorted(os.listdir(mask_dir))\n",
    "mask_arrays = []\n",
    "\n",
    "\n",
    "def load_masks_from_folder(mask_dir):\n",
    "    masks = []\n",
    "    for filename in os.listdir(mask_dir):\n",
    "        if filename.endswith('.jpeg'):\n",
    "            mask = Image.open(os.path.join(mask_dir, filename))\n",
    "            mask_arr = np.array(mask)\n",
    "            masks.append(mask_arr)\n",
    "    return masks\n",
    "\n",
    "mask_arrays = load_masks_from_folder(mask_dir)\n",
    "mask_arrays[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transform masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 64 x 64 x 3 arrays to 64 x 64 booleans for each instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_2dim(mask_arrays):\n",
    "    \n",
    "    # takes a lists of 3 dim numpy arrays, \n",
    "    # returns 2dim torch tensor in form of 0 for background, 1 for undamaged, 2 for damaged\n",
    "    masks_transformed = []\n",
    "    \n",
    "\n",
    "    # Define color thresholds for blue and magenta\n",
    "    blue_lower = np.array([0, 0, 100], dtype=np.uint8)\n",
    "    blue_upper = np.array([80, 80, 255], dtype=np.uint8)\n",
    "\n",
    "    magenta_lower = np.array([120, 0, 120], dtype=np.uint8)\n",
    "    magenta_upper = np.array([255, 100, 255], dtype=np.uint8)\n",
    "    \n",
    "    for mask_arr in mask_arrays: \n",
    "        \n",
    "        \n",
    "        # Create masks for blue and magenta regions\n",
    "        blue_mask = cv2.inRange(mask_arr, blue_lower, blue_upper)\n",
    "        magenta_mask = cv2.inRange(mask_arr, magenta_lower, magenta_upper)\n",
    "\n",
    "        # Combine the masks to get the final transformed mask\n",
    "        transformed_mask = np.zeros_like(blue_mask)\n",
    "        transformed_mask[blue_mask > 0] = 1  # Object 1 (Blue)\n",
    "        transformed_mask[magenta_mask > 0] = 2  # Object 2 (Magenta)\n",
    "        \n",
    "        masks_transformed.append(transformed_mask)\n",
    "        \n",
    "    # transform to torch tensor\n",
    "            \n",
    "    masks_array = np.array(masks_transformed)\n",
    "    masks_tensor = torch.from_numpy(masks_array)\n",
    "    \n",
    "    return masks_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_2dim_no_cv2(mask_arrays):\n",
    "    masks_transformed = []\n",
    "    \n",
    "    for mask_arr in mask_arrays:\n",
    "        # Define color thresholds for blue and magenta\n",
    "        blue_lower = np.array([0, 0, 100], dtype=np.uint8)\n",
    "        blue_upper = np.array([80, 80, 255], dtype=np.uint8)\n",
    "\n",
    "        magenta_lower = np.array([120, 0, 120], dtype=np.uint8)\n",
    "        magenta_upper = np.array([255, 100, 255], dtype=np.uint8)\n",
    "\n",
    "        # Create masks for blue and magenta regions\n",
    "        blue_mask = np.all((mask_arr >= blue_lower) & (mask_arr <= blue_upper), axis=-1)\n",
    "        magenta_mask = np.all((mask_arr >= magenta_lower) & (mask_arr <= magenta_upper), axis=-1)\n",
    "\n",
    "        # Create the transformed mask\n",
    "        transformed_mask = np.zeros_like(blue_mask, dtype=np.uint8)\n",
    "        transformed_mask[blue_mask] = 1  # Object 1 (Blue)\n",
    "        transformed_mask[magenta_mask] = 2  # Object 2 (Magenta)\n",
    "        \n",
    "        masks_transformed.append(transformed_mask)\n",
    "    \n",
    "    # Concatenate the transformed masks and convert to torch tensor\n",
    "    masks_tensor = torch.from_numpy(np.stack(masks_transformed, axis=0))\n",
    "    \n",
    "    return masks_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "masks_transformed = mask_to_2dim(mask_arrays)\n",
    "bbb = masks_transformed[100]\n",
    "\n",
    "masks_transformed2 = mask_to_2dim_no_cv2(mask_arrays)\n",
    "aaa = masks_transformed2[100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cull_if_no_bulding(masks_transformed, image_tensors):\n",
    "    cull_indexes = []\n",
    "    for i, tensor in enumerate(masks_transformed):\n",
    "        # Check if all elements in the tensor are zeros\n",
    "        if torch.all(tensor == 0):\n",
    "            cull_indexes.append(i)\n",
    "            \n",
    "    masks_culled = [mask for i, mask in enumerate(masks_transformed) if i not in cull_indexes]\n",
    "    image_culled = [image for i, image in enumerate(image_tensors) if i not in cull_indexes]\n",
    "    return masks_culled, image_culled\n",
    "\n",
    "\n",
    "masks_culled, image_culled = cull_if_no_bulding(masks_transformed, image_tensors)\n",
    "len(image_culled)\n",
    "\n",
    "masks_transformed, image_tensors = masks_culled.copy(), image_culled.copy()\n",
    "\n",
    "len(masks_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masks_2_dim_to_booleans(masks_transformed):\n",
    "    masks_binary = []\n",
    "\n",
    "    for mask in masks_transformed:\n",
    "            # use Connected Component Analysis to extract all objects from the image\n",
    "        \n",
    "            mask_np, count = ski.measure.label(mask, connectivity=1, return_num=True)\n",
    "            mask_test = torch.from_numpy(np.array(mask_np))\n",
    "\n",
    "            # We get the unique colors, as these would be the object ids.\n",
    "            mask_obj_ids = torch.unique(mask_test)\n",
    "\n",
    "            # first id is the background, so remove it.\n",
    "            mask_obj_ids = mask_obj_ids[1:]\n",
    "        \n",
    "            # split the color-encoded mask into a set of boolean masks.\n",
    "            mask_boolean = mask_test == mask_obj_ids[:, None, None]\n",
    "        \n",
    "            masks_binary.append(mask_boolean)\n",
    "    \n",
    "    return masks_binary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ...,  True,  True,  True],\n",
       "         [False, False, False,  ...,  True,  True,  True],\n",
       "         [False, False, False,  ...,  True,  True,  True]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks_binary = masks_2_dim_to_booleans(masks_transformed)\n",
    "masks_binary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_index_list(image_tensors):\n",
    "    num_images = len(image_tensors)\n",
    "    return list(range(num_images))\n",
    "\n",
    "image_ids = generate_index_list(image_tensors)\n",
    "len(image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = 11\n",
    "\n",
    "image = image_tensors[ind]\n",
    "mask = masks_binary[ind]\n",
    "annotated_tensor = draw_segmentation_masks( \n",
    "    image=(image*255).to(dtype=torch.uint8), \n",
    "    masks=mask, \n",
    "    alpha=0.3,\n",
    ")\n",
    "\n",
    "tensor_to_pil(annotated_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create bounding boxes with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_box(masks_transformed):\n",
    "    boxes = []\n",
    "    labels = []\n",
    "\n",
    "    for mask in masks_transformed:\n",
    "        labels_mask = []\n",
    "        standing_list = []\n",
    "        obj_ids = torch.unique(mask)\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        if len(obj_ids) == 1:\n",
    "            standing = mask == obj_ids[:, None, None]\n",
    "            standing_tensor = standing.int()\n",
    "            \n",
    "            \n",
    "        elif len(obj_ids) == 2:\n",
    "            separate_masks = mask == obj_ids[:, None, None]\n",
    "            standing, collapsed = torch.split(separate_masks, 1, dim=0)\n",
    "            standing_tensor, collapsed_tensor = standing.int(), collapsed.int()\n",
    "\n",
    "        ### standing buildings\n",
    "        standing_np, count = ski.measure.label(standing_tensor, connectivity=1, return_num=True)\n",
    "        standing_test = torch.from_numpy(np.array(standing_np))\n",
    "        standing_obj_ids = torch.unique(standing_test)\n",
    "        standing_obj_ids = standing_obj_ids[1:]\n",
    "        \n",
    "        standing_boolean = standing_test == standing_obj_ids[:, None, None]\n",
    "        standing_boxes_test = masks_to_boxes(standing_boolean)\n",
    "        \n",
    "        #cull mini boxes\n",
    "        \n",
    "        cull_list = []\n",
    "        for num, i in enumerate(standing_boxes_test):\n",
    "            result_x = i[0] - i[2]\n",
    "            result_y = i[1] - i[3]\n",
    "            if result_x == 0 or result_y == 0:\n",
    "                cull_list.append(num)\n",
    "            \n",
    "        cull_list.sort(reverse=True)\n",
    "        \n",
    "        for index in cull_list:\n",
    "            standing_boxes_test = torch.cat((standing_boxes_test[:index], standing_boxes_test[index + 1:]))\n",
    "        \n",
    "        # output: boxes and labels\n",
    "        label1 = 1\n",
    "        standing_list = [(row) for row in standing_boxes_test]\n",
    "\n",
    "        for i in range(len(standing_list)):\n",
    "            labels_mask.append(label1)\n",
    "            \n",
    "        ### collapsed buildings\n",
    "        collapsed_list = []\n",
    "\n",
    "        if len(obj_ids) == 2:  # Need to handle this condition\n",
    "            collapsed_np, count = ski.measure.label(collapsed_tensor, connectivity=1, return_num=True)\n",
    "            \n",
    "            collapsed_test = torch.from_numpy(np.array(collapsed_np))\n",
    "            \n",
    "            collapsed_obj_ids = torch.unique(collapsed_test)\n",
    "            collapsed_obj_ids = collapsed_obj_ids[1:]\n",
    "            \n",
    "            collapsed_boolean = collapsed_test == collapsed_obj_ids[:, None, None]\n",
    "            collapsed_boxes_test = masks_to_boxes(collapsed_boolean)\n",
    "            \n",
    "            #cull mini boxes\n",
    "        \n",
    "            cull_list = []\n",
    "            for num, i in enumerate(collapsed_boxes_test):\n",
    "                result_x = i[0] - i[2]\n",
    "                result_y = i[1] - i[3]\n",
    "                if result_x == 0 or result_y == 0:\n",
    "                    cull_list.append(num)\n",
    "            \n",
    "            cull_list.sort(reverse=True)\n",
    "        \n",
    "            for index in cull_list:\n",
    "                collapsed_boxes_test = torch.cat((collapsed_boxes_test[:index], collapsed_boxes_test[index + 1:]))\n",
    "            \n",
    "            # output: boxes and labels\n",
    "\n",
    "            label2 = 2\n",
    "            collapsed_list = [(row) for row in collapsed_boxes_test]\n",
    "            for i in range(len(collapsed_list)):\n",
    "                labels_mask.append(label2)\n",
    "                \n",
    "                \n",
    "\n",
    "        both_lists = standing_list + collapsed_list\n",
    "        boxes.append(both_lists)\n",
    "\n",
    "        boxes_int64 = []\n",
    "\n",
    "        for box in boxes:\n",
    "            tensor_box = torch.from_numpy(np.array(box)).to(torch.int64)\n",
    "            boxes_int64.append(tensor_box)\n",
    "\n",
    "        labels.append(labels_mask)\n",
    "\n",
    "        labels_int64 = []\n",
    "        for label in labels:\n",
    "            tensor_label = torch.from_numpy(np.array(label)).to(torch.int64)\n",
    "            labels_int64.append(tensor_label)\n",
    "\n",
    "    return boxes_int64, labels_int64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4, 38, 27, 61]]) tensor([2])\n"
     ]
    }
   ],
   "source": [
    "# takes masks_transformed not masks_binary\n",
    "labeled_boxes = mask_to_box(masks_transformed)\n",
    "boxes, labels = labeled_boxes\n",
    "ind = 105\n",
    "print(boxes[ind], labels[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(objects):\n",
    "    split_index = int(0.79 * len(objects))\n",
    "    # Split the list\n",
    "    train_dataset = objects[:split_index]\n",
    "    test_dataset = objects[split_index:]\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_images, test_images = test_train_split(image_tensors)\n",
    "train_masks, test_masks = test_train_split(masks_binary)\n",
    "train_boxes, test_boxes = test_train_split(boxes)\n",
    "train_labels, test_labels = test_train_split(labels)\n",
    "train_ids, test_ids = test_train_split(image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustData(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, masks, boxes, labels, image_ids):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.boxes = boxes\n",
    "        self.labels = labels\n",
    "        self.image_ids = image_ids\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # Retrieve the key for the image at the specified index\n",
    "        image_ids = self.image_ids[index]\n",
    "        mask = self.masks[index]\n",
    "        box = self.boxes[index]\n",
    "        label = self.labels[index]\n",
    "        image = self.images[index]       \n",
    "        \n",
    "        \n",
    "        return image, {'boxes': box,'labels': label, 'masks': mask} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = CustData(train_images, train_masks, train_boxes, train_labels, train_ids)\n",
    "test_dataset = CustData(test_images, test_masks, test_boxes, test_labels, test_ids)\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6,  0, 51, 33],\n",
      "        [37,  0, 47,  2],\n",
      "        [ 0, 24, 17, 42],\n",
      "        [ 0, 42, 31, 63],\n",
      "        [48, 55, 63, 63],\n",
      "        [50, 15, 63, 57]]) tensor([1, 1, 1, 1, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "index = 30\n",
    "print(train_dataset[index][1][\"boxes\"], train_dataset[index][1][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sample = train_dataset[index]\n",
    "\n",
    "# Get colors for dataset sample\n",
    "\n",
    "# Annotate the sample image with segmentation masks\n",
    "annotated_tensor = draw_segmentation_masks( \n",
    "    image=(dataset_sample[0]*255).to(dtype=torch.uint8), \n",
    "    masks=dataset_sample[1]['masks'], \n",
    "    alpha=0.3,\n",
    ")\n",
    "\n",
    "# Annotate the sample image with bounding boxes\n",
    "annotated_tensor = draw_bounding_boxes(\n",
    "    image=annotated_tensor, \n",
    "    boxes=dataset_sample[1]['boxes'],\n",
    ")\n",
    "\n",
    "tensor_to_pil(annotated_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T07:40:14.002783Z",
     "iopub.status.busy": "2023-03-27T07:40:14.002481Z",
     "iopub.status.idle": "2023-03-27T07:40:15.301352Z",
     "shell.execute_reply": "2023-03-27T07:40:15.300359Z",
     "shell.execute_reply.started": "2023-03-27T07:40:14.002755Z"
    }
   },
   "source": [
    "## 5. Make Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T07:40:15.303710Z",
     "iopub.status.busy": "2023-03-27T07:40:15.303310Z",
     "iopub.status.idle": "2023-03-27T07:40:15.308442Z",
     "shell.execute_reply": "2023-03-27T07:40:15.307414Z",
     "shell.execute_reply.started": "2023-03-27T07:40:15.303662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([[[0.4549, 0.4549, 0.4863,  ..., 0.2902, 0.2824, 0.2784],\n",
       "           [0.6039, 0.5922, 0.5804,  ..., 0.2902, 0.2745, 0.2706],\n",
       "           [0.6353, 0.6235, 0.6078,  ..., 0.2902, 0.2745, 0.2667],\n",
       "           ...,\n",
       "           [0.3294, 0.3255, 0.3373,  ..., 0.3843, 0.4118, 0.4471],\n",
       "           [0.3294, 0.3294, 0.3333,  ..., 0.3765, 0.3961, 0.4235],\n",
       "           [0.3333, 0.3255, 0.3216,  ..., 0.3686, 0.3725, 0.3843]],\n",
       "  \n",
       "          [[0.4431, 0.4431, 0.4745,  ..., 0.3333, 0.3373, 0.3373],\n",
       "           [0.5922, 0.5804, 0.5686,  ..., 0.3333, 0.3294, 0.3294],\n",
       "           [0.6235, 0.6118, 0.5961,  ..., 0.3333, 0.3294, 0.3216],\n",
       "           ...,\n",
       "           [0.3373, 0.3373, 0.3490,  ..., 0.4000, 0.4275, 0.4627],\n",
       "           [0.3412, 0.3412, 0.3529,  ..., 0.3922, 0.4118, 0.4392],\n",
       "           [0.3451, 0.3373, 0.3412,  ..., 0.3804, 0.3843, 0.4000]],\n",
       "  \n",
       "          [[0.3686, 0.3686, 0.4078,  ..., 0.2078, 0.2078, 0.2078],\n",
       "           [0.5176, 0.5059, 0.5020,  ..., 0.2078, 0.2000, 0.2000],\n",
       "           [0.5490, 0.5373, 0.5294,  ..., 0.2078, 0.2000, 0.1922],\n",
       "           ...,\n",
       "           [0.2549, 0.2549, 0.2745,  ..., 0.3098, 0.3373, 0.3725],\n",
       "           [0.2588, 0.2588, 0.2745,  ..., 0.2941, 0.3137, 0.3412],\n",
       "           [0.2627, 0.2549, 0.2627,  ..., 0.2745, 0.2784, 0.2941]]]),\n",
       "  {'boxes': tensor([[14, 37, 26, 53],\n",
       "           [52, 38, 63, 53],\n",
       "           [ 0,  0, 63, 39]]),\n",
       "   'labels': tensor([1, 1, 2]),\n",
       "   'masks': tensor([[[ True,  True,  True,  ...,  True,  True,  True],\n",
       "            [ True,  True,  True,  ...,  True,  True,  True],\n",
       "            [ True,  True,  True,  ...,  True,  True,  True],\n",
       "            ...,\n",
       "            [False, False, False,  ..., False, False, False],\n",
       "            [False, False, False,  ..., False, False, False],\n",
       "            [False, False, False,  ..., False, False, False]],\n",
       "   \n",
       "           [[False, False, False,  ..., False, False, False],\n",
       "            [False, False, False,  ..., False, False, False],\n",
       "            [False, False, False,  ..., False, False, False],\n",
       "            ...,\n",
       "            [False, False, False,  ..., False, False, False],\n",
       "            [False, False, False,  ..., False, False, False],\n",
       "            [False, False, False,  ..., False, False, False]],\n",
       "   \n",
       "           [[False, False, False,  ..., False, False, False],\n",
       "            [False, False, False,  ..., False, False, False],\n",
       "            [False, False, False,  ..., False, False, False],\n",
       "            ...,\n",
       "            [False, False, False,  ..., False, False, False],\n",
       "            [False, False, False,  ..., False, False, False],\n",
       "            [False, False, False,  ..., False, False, False]]])}],\n",
       " [tensor([[[0.2627, 0.2706, 0.2784,  ..., 0.2275, 0.2471, 0.2588],\n",
       "           [0.2863, 0.3020, 0.3098,  ..., 0.2431, 0.2471, 0.2392],\n",
       "           [0.3059, 0.3216, 0.3333,  ..., 0.2549, 0.2471, 0.2314],\n",
       "           ...,\n",
       "           [0.2392, 0.2431, 0.2431,  ..., 0.2392, 0.2353, 0.2275],\n",
       "           [0.2392, 0.2392, 0.2431,  ..., 0.2431, 0.2353, 0.2275],\n",
       "           [0.2118, 0.2078, 0.2078,  ..., 0.2392, 0.2235, 0.2118]],\n",
       "  \n",
       "          [[0.2784, 0.2863, 0.2863,  ..., 0.2471, 0.2667, 0.2784],\n",
       "           [0.3020, 0.3098, 0.3176,  ..., 0.2627, 0.2667, 0.2588],\n",
       "           [0.3137, 0.3294, 0.3412,  ..., 0.2745, 0.2667, 0.2510],\n",
       "           ...,\n",
       "           [0.2353, 0.2392, 0.2392,  ..., 0.2627, 0.2588, 0.2510],\n",
       "           [0.2510, 0.2510, 0.2549,  ..., 0.2667, 0.2588, 0.2510],\n",
       "           [0.2275, 0.2235, 0.2235,  ..., 0.2627, 0.2471, 0.2353]],\n",
       "  \n",
       "          [[0.1882, 0.1961, 0.2000,  ..., 0.1686, 0.1882, 0.2000],\n",
       "           [0.2118, 0.2235, 0.2314,  ..., 0.1843, 0.1882, 0.1804],\n",
       "           [0.2314, 0.2471, 0.2588,  ..., 0.1961, 0.1882, 0.1725],\n",
       "           ...,\n",
       "           [0.1647, 0.1686, 0.1686,  ..., 0.2078, 0.2039, 0.1961],\n",
       "           [0.1843, 0.1843, 0.1882,  ..., 0.2118, 0.2039, 0.1961],\n",
       "           [0.1608, 0.1569, 0.1569,  ..., 0.2078, 0.1922, 0.1804]]]),\n",
       "  {'boxes': tensor([[32,  0, 51,  3]]),\n",
       "   'labels': tensor([1]),\n",
       "   'masks': tensor([[[False, False, False,  ..., False, False, False],\n",
       "            [False, False, False,  ..., False, False, False],\n",
       "            [False, False, False,  ..., False, False, False],\n",
       "            ...,\n",
       "            [False, False, False,  ..., False, False, False],\n",
       "            [False, False, False,  ..., False, False, False],\n",
       "            [False, False, False,  ..., False, False, False]]])}]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the training batch size\n",
    "bs = 2\n",
    "\n",
    "# Set the number of worker processes for loading data ! multiprocessing.cpu_count()//2 !\n",
    "num_workers = 0\n",
    "\n",
    "# collatr function specifies the way data sticks together in a batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def custom_collate(data):\n",
    "  return data\n",
    "\n",
    "# Define parameters for DataLoader\n",
    "data_loader_params = {\n",
    "    'batch_size': bs,  # Batch size for data loading\n",
    "    'num_workers': num_workers,  # Number of subprocesses to use for data loading\n",
    "    'collate_fn': custom_collate,\n",
    "    'pin_memory': True,\n",
    "    'pin_memory_device': device\n",
    "}\n",
    "\n",
    "# Create DataLoader for training data. Data is shuffled for every epoch.\n",
    "train_dataloader = DataLoader(train_dataset, **data_loader_params, shuffle=True)\n",
    "\n",
    "# Create DataLoader for validation data. Shuffling is not necessary for validation data.\n",
    "test_dataloader = DataLoader(test_dataset, **data_loader_params)\n",
    "\n",
    "\n",
    "pd.Series({\n",
    "    'Number of batches in train DataLoader:': len(train_dataloader),\n",
    "    'Number of batches in validation DataLoader:': len(test_dataloader)}\n",
    ").to_frame().style.hide(axis='columns')\n",
    "\n",
    "\n",
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Loading the Mask R-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 3\n",
    "\n",
    "# Initialize a Mask R-CNN model with pretrained weights\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "    )\n",
       "    (mask_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)\n",
       "    (mask_head): MaskRCNNHeads(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (mask_predictor): MaskRCNNPredictor(\n",
       "      (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask_fcn_logits): Conv2d(256, 91, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 3\n",
    "\n",
    "# Initialize a Mask R-CNN model with pretrained weights\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn()\n",
    "\n",
    "# Get the number of input features for the classifier\n",
    "in_features_box = model.roi_heads.box_predictor.cls_score.in_features\n",
    "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "\n",
    "# Get the numbner of output channels for the Mask Predictor\n",
    "dim_reduced = model.roi_heads.mask_predictor.conv5_mask.out_channels\n",
    "\n",
    "# Replace the box predictor\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_channels=in_features_box, num_classes=classes)\n",
    "\n",
    "hidden_layer = 256\n",
    "\n",
    "# Replace the mask predictor\n",
    "model.roi_heads.mask_predictor = MaskRCNNPredictor(in_channels=in_features_mask, dim_reduced=hidden_layer, num_classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model's device and data type\n",
    "model.to(device=device, dtype=dtype);\n",
    "\n",
    "# Add attributes to store the device and model name for later reference\n",
    "model.device = device\n",
    "model.name = 'maskrcnn_resnet50_fpn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Summarize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kubaw\\miniforge3\\envs\\pytorch-env\\lib\\site-packages\\torchtnt\\utils\\module_summary.py:271: UserWarning: Backward FLOPs are only computed if module foward returns a tensor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th># Parameters</th>\n",
       "      <th># Trainable Parameters</th>\n",
       "      <th>Size (bytes)</th>\n",
       "      <th>Forward FLOPs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MaskRCNN</td>\n",
       "      <td>43.9 M</td>\n",
       "      <td>43.7 M</td>\n",
       "      <td>176 M</td>\n",
       "      <td>185 G</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Type # Parameters # Trainable Parameters Size (bytes) Forward FLOPs\n",
       "0  MaskRCNN       43.9 M                 43.7 M        176 M         185 G"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_summarize(model):\n",
    "\n",
    "    test_inp = torch.randn(1, 3, 256, 256).to(device)\n",
    "\n",
    "    summary_df = markdown_to_pandas(f\"{get_module_summary(model.eval(), [test_inp])}\")\n",
    "\n",
    "    # # Filter the summary to only contain Conv2d layers and the model\n",
    "    summary_df = summary_df[summary_df.index == 0]\n",
    "\n",
    "    return summary_df.drop(['In size', 'Out size', 'Contains Uninitialized Parameters?'], axis=1)\n",
    "\n",
    "model_summarize(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T07:46:27.157813Z",
     "iopub.status.busy": "2023-03-27T07:46:27.157375Z",
     "iopub.status.idle": "2023-03-27T07:46:27.164342Z",
     "shell.execute_reply": "2023-03-27T07:46:27.163373Z",
     "shell.execute_reply.started": "2023-03-27T07:46:27.157776Z"
    }
   },
   "source": [
    "## 8. Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T07:46:29.813497Z",
     "iopub.status.busy": "2023-03-27T07:46:29.813140Z",
     "iopub.status.idle": "2023-03-27T07:46:29.820564Z",
     "shell.execute_reply": "2023-03-27T07:46:29.819568Z",
     "shell.execute_reply.started": "2023-03-27T07:46:29.813467Z"
    }
   },
   "source": [
    "### 8.1 Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T07:46:29.960000Z",
     "iopub.status.busy": "2023-03-27T07:46:29.959690Z",
     "iopub.status.idle": "2023-03-27T07:46:29.974260Z",
     "shell.execute_reply": "2023-03-27T07:46:29.973392Z",
     "shell.execute_reply.started": "2023-03-27T07:46:29.959968Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T07:46:30.668035Z",
     "iopub.status.busy": "2023-03-27T07:46:30.667701Z",
     "iopub.status.idle": "2023-03-27T07:46:30.673224Z",
     "shell.execute_reply": "2023-03-27T07:46:30.672206Z",
     "shell.execute_reply.started": "2023-03-27T07:46:30.668006Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T07:40:22.015025Z",
     "iopub.status.busy": "2023-03-27T07:40:22.012780Z",
     "iopub.status.idle": "2023-03-27T07:40:22.021288Z",
     "shell.execute_reply": "2023-03-27T07:40:22.020491Z",
     "shell.execute_reply.started": "2023-03-27T07:40:22.014972Z"
    }
   },
   "source": [
    "### 8.2 Set the Model Checkpoint Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T07:50:21.522384Z",
     "iopub.status.busy": "2023-03-27T07:50:21.522023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\pytorch-buildings-maskrcnn\\2023-10-18_16-04-22\\maskrcnn_resnet50_fpn.pth\n"
     ]
    }
   ],
   "source": [
    "# Generate timestamp for the training session (Year-Month-Day_Hour_Minute_Second)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Create a directory to store the checkpoints if it does not already exist\n",
    "checkpoint_dir = Path(project_dir/f\"{timestamp}\")\n",
    "\n",
    "# Create the checkpoint directory if it does not already exist\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# The model checkpoint path\n",
    "checkpoint_path = checkpoint_dir/f\"{model.name}.pth\"\n",
    "\n",
    "print(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T05:32:00.264708Z",
     "iopub.status.busy": "2023-03-26T05:32:00.264167Z",
     "iopub.status.idle": "2023-03-26T05:32:00.483405Z",
     "shell.execute_reply": "2023-03-26T05:32:00.482438Z",
     "shell.execute_reply.started": "2023-03-26T05:32:00.264657Z"
    }
   },
   "source": [
    "### 8.3 Configure the Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T05:32:03.705505Z",
     "iopub.status.busy": "2023-03-26T05:32:03.705112Z",
     "iopub.status.idle": "2023-03-26T05:32:03.882450Z",
     "shell.execute_reply": "2023-03-26T05:32:03.881409Z",
     "shell.execute_reply.started": "2023-03-26T05:32:03.705454Z"
    }
   },
   "outputs": [],
   "source": [
    "# Learning rate for the model\n",
    "\n",
    "# Number of training epochs\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# AdamW optimizer; includes weight decay for regularization\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T05:32:13.441050Z",
     "iopub.status.busy": "2023-03-26T05:32:13.440353Z",
     "iopub.status.idle": "2023-03-26T05:32:16.446381Z",
     "shell.execute_reply": "2023-03-26T05:32:16.444816Z",
     "shell.execute_reply.started": "2023-03-26T05:32:13.440990Z"
    }
   },
   "source": [
    "## 9. Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im here\n",
      "{'loss_classifier': tensor(1.0826, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>), 'loss_mask': tensor(1.5802, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_objectness': tensor(0.6897, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.0182, device='cuda:0', grad_fn=<DivBackward0>)}\n",
      "epoch: 0    training loss 76.76234710216522    validation loss: 17.161327123641968\n",
      "im here\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 24\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#TRAINING LOOP\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# dt is for training the dataloader, \u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i , dt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m     22\u001b[0m     \n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m#imgs to device\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m [\u001b[43mdt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m , dt[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)]\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# targets list will contain dictionaries where the values from targ are moved to a specified device\u001b[39;00m\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;66;03m# Iterates over each element (t) in the targ list\u001b[39;00m\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;66;03m# Creates a dictionary by iterating over the key-value pairs in t \u001b[39;00m\n\u001b[0;32m     29\u001b[0m     targ \u001b[38;5;241m=\u001b[39m [dt[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m] , dt[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_train_losses = []\n",
    "all_val_losses = []\n",
    "\n",
    "\n",
    "flag = False\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_epoch_loss = 0\n",
    "    val_epoch_loss = 0\n",
    "    \n",
    "    # put the model into training mode\n",
    "    model.train()\n",
    "    \n",
    "    print(\"im here\")\n",
    "    \n",
    "    \n",
    "    #TRAINING LOOP\n",
    "    \n",
    "    # dt is for training the dataloader, \n",
    "    for i , dt in enumerate(train_dataloader):\n",
    "        \n",
    "        #imgs to device\n",
    "        imgs = [dt[0][0].to(device) , dt[1][0].to(device)]\n",
    "        \n",
    "        # targets list will contain dictionaries where the values from targ are moved to a specified device\n",
    "            # Iterates over each element (t) in the targ list\n",
    "            # Creates a dictionary by iterating over the key-value pairs in t \n",
    "        targ = [dt[0][1] , dt[1][1]]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targ]\n",
    "        \n",
    "        # CALCULATE LOSS: for boxes, masks and the region proposal network\n",
    "        loss = model(imgs, targets)\n",
    "        \n",
    "        # flag ensures that the initial parameters of the loss are only printed once\n",
    "        if not flag:\n",
    "            print(loss)\n",
    "            flag = True\n",
    "        \n",
    "        # sum all the losses\n",
    "        losses = sum([l for l in loss.values()])\n",
    "        \n",
    "        # add the losses into train_epoch_loss for one epoch\n",
    "        train_epoch_loss += losses.cpu().detach().numpy()\n",
    "        \n",
    "        # optimizer zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # backpropagation\n",
    "        losses.backward()\n",
    "        \n",
    "        # optimize weights and biases\n",
    "        optimizer.step()\n",
    "        \n",
    "    # summrize all losses\n",
    "    all_train_losses.append(train_epoch_loss)\n",
    "    \n",
    "    #VALIDATION LOOP\n",
    "    \n",
    "    # put model into inference mode\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for j , dt in enumerate(test_dataloader):\n",
    "            \n",
    "            #imgs to device\n",
    "            imgs = [dt[0][0].to(device) , dt[1][0].to(device)]\n",
    "            \n",
    "            # targets list will contain dictionaries where the values from targ are moved to a specified device\n",
    "                # Iterates over each element (t) in the targ list\n",
    "                # Creates a dictionary by iterating over the key-value pairs in t     \n",
    "            targ = [dt[0][1] , dt[1][1]]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targ]\n",
    "            \n",
    "            # CALCULATE LOSS: for boxes, masks and the region proposal network\n",
    "            loss = model(imgs , targets)\n",
    "            losses = sum([l for l in loss.values()])\n",
    "            \n",
    "            # add the losses into val_epoch_loss for one epoch\n",
    "            val_epoch_loss += losses.cpu().detach().numpy()\n",
    "            \n",
    "        # summrize all losses\n",
    "        all_val_losses.append(val_epoch_loss)\n",
    "      \n",
    "    \n",
    "    #print results for on epoch\n",
    "    print(f\"epoch: {epoch}    training loss {train_epoch_loss}    validation loss: {val_epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2aa0ae59c30>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(all_train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16.176132678985596,\n",
       " 16.297901332378387,\n",
       " 16.567949533462524,\n",
       " 15.741186559200287,\n",
       " 16.590915620326996,\n",
       " 15.951047480106354,\n",
       " 16.021022260189056,\n",
       " 15.419067978858948,\n",
       " 15.376284122467041,\n",
       " 16.46754091978073]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
