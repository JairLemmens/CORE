{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e68e8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting v_0_1_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile v_0_1_model.py\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "from torchtnt.utils import get_module_summary\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from cjm_pandas_utils.core import markdown_to_pandas, convert_to_numeric, convert_to_string\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn_v2, MaskRCNN\n",
    "from torchvision.models.detection import MaskRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "\n",
    "def model_summarize(model):\n",
    "\n",
    "    test_inp = torch.randn(1, 3, 256, 256).to(device)\n",
    "\n",
    "    summary_df = markdown_to_pandas(f\"{get_module_summary(model.eval(), [test_inp])}\")\n",
    "\n",
    "    # # Filter the summary to only contain Conv2d layers and the model\n",
    "    summary_df = summary_df[summary_df.index == 0]\n",
    "\n",
    "    return summary_df.drop(['In size', 'Out size', 'Contains Uninitialized Parameters?'], axis=1)\n",
    "\n",
    "\n",
    "# the following fx performs a single pass through the training set\n",
    "\n",
    "def run_epoch(model, dataloader, optimizer, lr_scheduler, device, scaler, is_training):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to run a single training or evaluation epoch.\n",
    "    \n",
    "    IN:\n",
    "        model: A PyTorch model to train or evaluate.\n",
    "        dataloader: A PyTorch DataLoader providing the data.\n",
    "        optimizer: The optimizer to use for training the model.\n",
    "        loss_func: The loss function used for training.\n",
    "        device: The device (CPU or GPU) to run the model on.\n",
    "        scaler: Gradient scaler for mixed-precision training.\n",
    "        is_training: Boolean flag indicating whether the model is in training or evaluation mode.\n",
    "    \n",
    "    Returns:\n",
    "        The average loss for the epoch.\n",
    "        \"\"\"\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0  # Initialize the total loss for this epoch\n",
    "    progress_bar = tqdm(total=len(dataloader), desc=\"Train\" if is_training else \"Eval\")  # Initialize a progress bar\n",
    "    \n",
    "    # Loop over the data\n",
    "    for batch_id, (inputs, targets) in enumerate(dataloader):\n",
    "        # Move inputs and targets to the specified device\n",
    "        inputs = torch.stack(inputs).to(device)\n",
    "        \n",
    "        # Forward pass with Automatic Mixed Precision (AMP) context manager\n",
    "        # provides convenience methods where some operations use the float32 datatype and other operations use float16\n",
    "        with autocast(torch.device(device).type):\n",
    "            if is_training:\n",
    "                losses = model(inputs.to(device), move_data_to_device(targets, device))\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    losses = model(inputs.to(device), move_data_to_device(targets, device))\n",
    "                    \n",
    "            # Compute the loss\n",
    "            loss = sum([loss for loss in losses.values()])  # Sum up the losses\n",
    "\n",
    "        # If in training mode, backpropagate the error and update the weights\n",
    "        if is_training:\n",
    "            if scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                old_scaler = scaler.get_scale()\n",
    "                scaler.update()\n",
    "                new_scaler = scaler.get_scale()\n",
    "                if new_scaler >= old_scaler:\n",
    "                    lr_scheduler.step()\n",
    "                    \n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        # Update the total loss\n",
    "        loss_item = loss.item()\n",
    "        epoch_loss += loss_item\n",
    "        \n",
    "        # Update the progress bar\n",
    "        progress_bar_dict = dict(loss=loss_item, avg_loss=epoch_loss/(batch_id+1))\n",
    "        if is_training:\n",
    "            progress_bar_dict.update(lr=lr_scheduler.get_last_lr()[0])\n",
    "        progress_bar.set_postfix(progress_bar_dict)\n",
    "        progress_bar.update()\n",
    "        \n",
    "        # If the loss is NaN or infinite, stop the training/evaluation process\n",
    "        if math.isnan(loss_item) or math.isinf(loss_item):\n",
    "            print(f\"Loss is NaN or infinite at batch {batch_id}. Stopping {'training' if is_training else 'evaluation'}.\")\n",
    "            break\n",
    "    # Cleanup and close the progress bar \n",
    "    progress_bar.close()\n",
    "    \n",
    "    # Return the average loss for this epoch\n",
    "    return epoch_loss / (batch_id + 1)\n",
    "\n",
    "\n",
    "def train_loop(model, \n",
    "               train_dataloader, \n",
    "               valid_dataloader, \n",
    "               optimizer,  \n",
    "               lr_scheduler, \n",
    "               device, \n",
    "               epochs, \n",
    "               checkpoint_path, \n",
    "               use_scaler=False):\n",
    "    \"\"\"\n",
    "    Main training loop.\n",
    "    \n",
    "    Args:\n",
    "        model: A PyTorch model to train.\n",
    "        train_dataloader: A PyTorch DataLoader providing the training data.\n",
    "        valid_dataloader: A PyTorch DataLoader providing the validation data.\n",
    "        optimizer: The optimizer to use for training the model.\n",
    "        lr_scheduler: The learning rate scheduler.\n",
    "        device: The device (CPU or GPU) to run the model on.\n",
    "        epochs: The number of epochs to train for.\n",
    "        checkpoint_path: The path where to save the best model checkpoint.\n",
    "        use_scaler: Whether to scale graidents when using a CUDA device\n",
    "        \n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "    \n",
    "    # Initialize a gradient scaler for mixed-precision training if the device is a CUDA GPU\n",
    "    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' and use_scaler else None\n",
    "    best_loss = float('inf')  # Initialize the best validation loss\n",
    "    \n",
    "    \n",
    "    # Loop over the epochs\n",
    "    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "        # Run a training epoch and get the training loss\n",
    "        train_loss = run_epoch(model, train_dataloader, optimizer, lr_scheduler, device, scaler, is_training=True)\n",
    "        # Run an evaluation epoch and get the validation loss\n",
    "        with torch.no_grad():\n",
    "            valid_loss = run_epoch(model, valid_dataloader, None, None, device, scaler, is_training=False)\n",
    "\n",
    "\n",
    "                    # If the validation loss is lower than the best validation loss seen so far, save the model checkpoint\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "            # Save metadata about the training process\n",
    "            training_metadata = {\n",
    "                'epoch': epoch,\n",
    "                'train_loss': train_loss,\n",
    "                'valid_loss': valid_loss, \n",
    "                'learning_rate': lr_scheduler.get_last_lr()[0],\n",
    "                'model_architecture': model.name\n",
    "            }\n",
    "            with open(Path(checkpoint_path.parent/'training_metadata.json'), 'w') as f:\n",
    "                json.dump(training_metadata, f)\n",
    "                \n",
    "        # If the training or validation loss is NaN or infinite, stop the training process\n",
    "        if any(math.isnan(loss) or math.isinf(loss) for loss in [train_loss, valid_loss]):\n",
    "            print(f\"Loss is NaN or infinite at epoch {epoch}. Stopping training.\")\n",
    "            break\n",
    "\n",
    "    # If the device is a GPU, empty the cache\n",
    "    if device.type != 'cpu':\n",
    "        getattr(torch, device.type).empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fbcc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "763a946c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3220876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e9ae86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b6a94f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
