{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T07:40:10.510850Z",
     "iopub.status.busy": "2023-03-27T07:40:10.510375Z",
     "iopub.status.idle": "2023-03-27T07:40:12.528437Z",
     "shell.execute_reply": "2023-03-27T07:40:12.527481Z",
     "shell.execute_reply.started": "2023-03-27T07:40:10.510804Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import datetime\n",
    "from functools import partial\n",
    "from glob import glob\n",
    "import math\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "import random\n",
    "from typing import Any, Dict, Optional\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.amp import autocast # allows for differentr datatypes (torch.32/ torch.16)\n",
    "from torchvision import transforms as T\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from cjm_pytorch_utils.core import pil_to_tensor, tensor_to_pil, get_torch_device, set_seed, denorm_img_tensor, move_data_to_device\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "from cjm_psl_utils.core import download_file, file_extract, get_source_code\n",
    "from cjm_pandas_utils.core import markdown_to_pandas, convert_to_numeric, convert_to_string\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtnt.utils import get_module_summary\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import imageio.v3 as iio\n",
    "import ipympl\n",
    "import skimage as ski\n",
    "\n",
    "# Import Mask R-CNN\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn_v2, MaskRCNN\n",
    "from torchvision.models.detection import MaskRCNN_ResNet50_FPN_V2_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_torch_device()\n",
    "dtype = torch.float32\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up projects's directory\n",
    "# project's name\n",
    "project_name = f\"pytorch-buildings-maskrcnn\"\n",
    "\n",
    "# path for project's folder\n",
    "project_dir = Path(f\"/{project_name}/\")\n",
    "\n",
    "# create the project directory\n",
    "project_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T07:40:12.994007Z",
     "iopub.status.busy": "2023-03-27T07:40:12.993495Z",
     "iopub.status.idle": "2023-03-27T07:40:13.162589Z",
     "shell.execute_reply": "2023-03-27T07:40:13.161654Z",
     "shell.execute_reply.started": "2023-03-27T07:40:12.993973Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load images\n",
    "\n",
    "images_dir = \"C:/Users/kubaw/Desktop/DELFT/CORE/satellite_predictions/Dataset2/Images/Destroyed\"  \n",
    "image_filenames = sorted(os.listdir(images_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply transform to the images (normalise)\n",
    "def load_images_from_folder(images_dir):\n",
    "    images = []\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "    for filename in os.listdir(images_dir):\n",
    "        if filename.endswith('.jpeg'):\n",
    "            img = Image.open(os.path.join(images_dir, filename))\n",
    "            img_t = transform(img)\n",
    "            images.append(img_t)\n",
    "    return images\n",
    "\n",
    "image_tensors = load_images_from_folder(images_dir)\n",
    "\n",
    "len(image_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Make image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_index_list(image_tensors):\n",
    "    num_images = len(image_tensors)\n",
    "    return list(range(num_images))\n",
    "\n",
    "image_ids = generate_index_list(image_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Load Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_dir = \"C:/Users/kubaw/Desktop/DELFT/CORE/satellite_predictions/Dataset2/Masks/Destroyed\"  \n",
    "mask_filenames = sorted(os.listdir(mask_dir))\n",
    "mask_arrays = []\n",
    "\n",
    "\n",
    "def load_masks_from_folder(mask_dir):\n",
    "    masks = []\n",
    "    for filename in os.listdir(mask_dir):\n",
    "        if filename.endswith('.jpeg'):\n",
    "            mask = Image.open(os.path.join(mask_dir, filename))\n",
    "            mask_arr = np.array(mask)\n",
    "            masks.append(mask_arr)\n",
    "    return masks\n",
    "\n",
    "mask_arrays = load_masks_from_folder(mask_dir)\n",
    "mask_arrays[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transform masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 64 x 64 x 3 arrays to 64 x 64 booleans for each instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_2dim(mask_arrays):\n",
    "    \n",
    "    # takes a lists of 3 dim numpy arrays, \n",
    "    # returns 2dim torch tensor in form of 0 for background, 1 for undamaged, 2 for damaged\n",
    "    masks_transformed = []\n",
    "    \n",
    "\n",
    "    # Define color thresholds for blue and magenta\n",
    "    blue_lower = np.array([0, 0, 100], dtype=np.uint8)\n",
    "    blue_upper = np.array([80, 80, 255], dtype=np.uint8)\n",
    "\n",
    "    magenta_lower = np.array([120, 0, 120], dtype=np.uint8)\n",
    "    magenta_upper = np.array([255, 100, 255], dtype=np.uint8)\n",
    "    \n",
    "    for mask_arr in mask_arrays: \n",
    "        \n",
    "        \n",
    "        # Create masks for blue and magenta regions\n",
    "        blue_mask = cv2.inRange(mask_arr, blue_lower, blue_upper)\n",
    "        magenta_mask = cv2.inRange(mask_arr, magenta_lower, magenta_upper)\n",
    "\n",
    "        # Combine the masks to get the final transformed mask\n",
    "        transformed_mask = np.zeros_like(blue_mask)\n",
    "        transformed_mask[blue_mask > 0] = 1  # Object 1 (Blue)\n",
    "        transformed_mask[magenta_mask > 0] = 2  # Object 2 (Magenta)\n",
    "        \n",
    "        masks_transformed.append(transformed_mask)\n",
    "        \n",
    "    # transform to torch tensor\n",
    "            \n",
    "    masks_array = np.array(masks_transformed)\n",
    "    masks_tensor = torch.from_numpy(masks_array)\n",
    "    \n",
    "    return masks_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks_transformed = mask_to_2dim(mask_arrays)\n",
    "masks_transformed[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masks_2_dim_to_booleans(masks_transformed):\n",
    "    masks_binary = []\n",
    "\n",
    "    for mask in masks_transformed:\n",
    "            # use Connected Component Analysis to extract all objects from the image\n",
    "        \n",
    "            mask_np, count = ski.measure.label(mask, connectivity=1, return_num=True)\n",
    "            mask_test = torch.from_numpy(np.array(mask_np))\n",
    "\n",
    "            # We get the unique colors, as these would be the object ids.\n",
    "            mask_obj_ids = torch.unique(mask_test)\n",
    "\n",
    "            # first id is the background, so remove it.\n",
    "            mask_obj_ids = mask_obj_ids[1:]\n",
    "        \n",
    "            # split the color-encoded mask into a set of boolean masks.\n",
    "            mask_boolean = mask_test == mask_obj_ids[:, None, None]\n",
    "        \n",
    "            masks_binary.append(mask_boolean)\n",
    "    \n",
    "    return masks_binary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 64])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks_binary = masks_2_dim_to_booleans(masks_transformed)\n",
    "masks_binary[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create bounding boxes with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_box(masks_transformed):\n",
    "    \n",
    "    \"\"\"\n",
    "    args: 3 dim array of image masks \n",
    "    returns: tuple of bounding boxes, tuple of boxes labels\n",
    "    \"\"\"\n",
    "    \n",
    "    boxes = []\n",
    "    labels = []\n",
    "\n",
    "    for mask in masks_transformed:\n",
    "        \n",
    "        labels_mask = []\n",
    "        \n",
    "        ### we split the masks into damaged and andamaged tessors\n",
    "        standing_list = []\n",
    "        # We get the unique colors, as these would be the object ids.\n",
    "        obj_ids = torch.unique(mask)\n",
    "\n",
    "        # first id is the background, so remove it.\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        if len(obj_ids) == 1:\n",
    "\n",
    "            # split the color-encoded mask into a set of boolean masks.\n",
    "            standing = mask == obj_ids[:, None, None]\n",
    "            standing_tensor = standing.int()\n",
    "    \n",
    "        elif len(obj_ids) == 2:\n",
    "    \n",
    "            separate_masks = mask == obj_ids[:, None, None]\n",
    "            standing, collapsed = torch.split(separate_masks, 1, dim=0)\n",
    "            standing_tensor, collapsed_tensor = standing.int(), collapsed.int()\n",
    "        \n",
    "        ### standing buildings\n",
    "        \n",
    "        # use Connected Component Analysis to extract all objects from the image\n",
    "\n",
    "        standing_np, count = ski.measure.label(standing_tensor, connectivity=1, return_num=True)\n",
    "        standing_test = torch.from_numpy(np.array(standing_np))\n",
    "\n",
    "        # We get the unique colors, as these would be the object ids.\n",
    "        standing_obj_ids = torch.unique(standing_test)\n",
    "\n",
    "        # first id is the background, so remove it.\n",
    "        standing_obj_ids = standing_obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set of boolean masks.\n",
    "        standing_boolean = standing_test == standing_obj_ids[:, None, None]\n",
    "        \n",
    "        #make boxes (x1, x2, y1, y2)\n",
    "        standing_boxes_test = masks_to_boxes(standing_boolean)\n",
    "        \n",
    "        #label standing boxes\n",
    "\n",
    "        label1 = 1\n",
    "        standing_list = [(row) for row in standing_boxes_test]\n",
    "\n",
    "        for i in range(len(standing_list)):\n",
    "            labels_mask.append(label1)\n",
    "\n",
    "        \n",
    "        ### collapsed buildings\n",
    "        \n",
    "        collapsed_list = []\n",
    "        \n",
    "        # use Connected Component Analysis to extract all objects from the image\n",
    "        \n",
    "        collapsed_np, count = ski.measure.label(collapsed_tensor, connectivity=1, return_num=True)\n",
    "        collapsed_test = torch.from_numpy(np.array(collapsed_np))\n",
    "\n",
    "        # We get the unique colors, as these would be the object ids.\n",
    "        collapsed_obj_ids = torch.unique(collapsed_test)\n",
    "\n",
    "        # first id is the background, so remove it.\n",
    "        collapsed_obj_ids = collapsed_obj_ids[1:]\n",
    "        \n",
    "        # split the color-encoded mask into a set of boolean masks.\n",
    "        collapsed_boolean = collapsed_test == collapsed_obj_ids[:, None, None]\n",
    "        \n",
    "        #make boxes (x1, x2, y1, y2)\n",
    "        collapsed_boxes_test = masks_to_boxes(collapsed_boolean)\n",
    "        \n",
    "        #make tuple\n",
    "        collapsed_list = [(row) for row in collapsed_boxes_test]\n",
    "\n",
    "        #label collapsed boxes\n",
    "\n",
    "        label2 = 2\n",
    "        collapsed_list = [(row) for row in collapsed_boxes_test]\n",
    "        \n",
    "        for i in range(len(collapsed_list)):\n",
    "            labels_mask.append(label2)\n",
    "        \n",
    "        \n",
    "        ### append boxes list\n",
    "        both_lists = standing_list + collapsed_list\n",
    "        boxes.append(both_lists)\n",
    "        \n",
    "        # make boxes to torch.int64 datatype\n",
    "        \n",
    "        boxes_int64 = []\n",
    "        \n",
    "        for box in boxes:\n",
    "            \n",
    "            tensor_box = torch.from_numpy(np.array(box)).to(torch.int64)\n",
    "            boxes_int64.append(tensor_box)\n",
    "\n",
    "        \n",
    "        ### append labels list\n",
    "        \n",
    "        labels.append(labels_mask)\n",
    "\n",
    "        labels_int64 = []\n",
    "        for label in labels:\n",
    "            \n",
    "            tensor_label = torch.from_numpy(np.array(label)).to(torch.int64)\n",
    "            labels_int64.append(tensor_label)        \n",
    "        \n",
    "        \n",
    "    return boxes_int64, labels_int64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_boxes = mask_to_box(masks_transformed)\n",
    "boxes, labels = labeled_boxes\n",
    "boxes[1].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Concatinate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## SO FAR WE HAVE\n",
    "\n",
    "# boxes\n",
    "len(boxes)\n",
    "\n",
    "# labels\n",
    "len(labels)\n",
    "\n",
    "# masks\n",
    "len(masks_binary)\n",
    "\n",
    "# image_ids\n",
    "len(image_ids)\n",
    "\n",
    "# images \n",
    "len(image_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatinate_data(images, boxes, masks, labels): \n",
    "    # IN: image tensors, box tensors, binary mask tensors, box labels tensor\n",
    "    # OUT: list of the image tensor and dictionary of target values at each index\n",
    "    keys = ['boxes', 'masks', 'labels']\n",
    "    dataset_targets = []\n",
    "    for b, l, m in zip(boxes, labels, masks):\n",
    "    \n",
    "        data_dict = {\n",
    "            'boxes': b,\n",
    "            'labels': l,\n",
    "            'masks': m\n",
    "        }\n",
    "        dataset_targets.append(data_dict)\n",
    "        \n",
    "    dataset_fin = []\n",
    "\n",
    "    for i in range(len(dataset_targets)):\n",
    "        # Create a tuple with the dictionary and the array\n",
    "        combined_data = (images[i], dataset_targets[i])\n",
    "        dataset_fin.append(combined_data)    \n",
    "    \n",
    "    return dataset_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boxes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset_fin \u001b[38;5;241m=\u001b[39m concatinate_data(image_tensors, \u001b[43mboxes\u001b[49m, masks_binary, labels)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'boxes' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_fin = concatinate_data(image_tensors, boxes, masks_binary, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_train_split(dataset):\n",
    "    # Shuffle the list randomly\n",
    "    random.shuffle(dataset_fin)\n",
    "    dataset_sh = dataset_fin.copy()\n",
    "\n",
    "    split_index = int(0.75 * len(dataset_sh))\n",
    "\n",
    "    # Split the list\n",
    "    train_dataset = dataset_sh[:split_index]\n",
    "    test_dataset = dataset_sh[split_index:]\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_dataset, test_dataset = test_train_split(dataset_fin)\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T07:40:14.002783Z",
     "iopub.status.busy": "2023-03-27T07:40:14.002481Z",
     "iopub.status.idle": "2023-03-27T07:40:15.301352Z",
     "shell.execute_reply": "2023-03-27T07:40:15.300359Z",
     "shell.execute_reply.started": "2023-03-27T07:40:14.002755Z"
    }
   },
   "source": [
    "## 5. Make Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T07:40:15.303710Z",
     "iopub.status.busy": "2023-03-27T07:40:15.303310Z",
     "iopub.status.idle": "2023-03-27T07:40:15.308442Z",
     "shell.execute_reply": "2023-03-27T07:40:15.307414Z",
     "shell.execute_reply.started": "2023-03-27T07:40:15.303662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_f85e6\">\n",
       "  <thead>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f85e6_level0_row0\" class=\"row_heading level0 row0\" >Number of batches in train DataLoader:</th>\n",
       "      <td id=\"T_f85e6_row0_col0\" class=\"data row0 col0\" >9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f85e6_level0_row1\" class=\"row_heading level0 row1\" >Number of batches in validation DataLoader:</th>\n",
       "      <td id=\"T_f85e6_row1_col0\" class=\"data row1 col0\" >3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x223dea80340>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the training batch size\n",
    "bs = 10\n",
    "\n",
    "# Set the number of worker processes for loading data ! multiprocessing.cpu_count()//2 !\n",
    "num_workers = multiprocessing.cpu_count()//2\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Define parameters for DataLoader\n",
    "data_loader_params = {\n",
    "    'batch_size': bs,  # Batch size for data loading\n",
    "    'num_workers': num_workers,  # Number of subprocesses to use for data loading\n",
    "    'collate_fn': collate_fn,\n",
    "    'pin_memory': True,\n",
    "    'pin_memory_device': device\n",
    "}\n",
    "\n",
    "#Create DataLoader for training data. Data is shuffled for every epoch.\n",
    "train_dataloader = DataLoader(train_dataset, **data_loader_params, shuffle=True)\n",
    "\n",
    "# Create DataLoader for validation data. Shuffling is not necessary for validation data.\n",
    "valid_dataloader = DataLoader(test_dataset, **data_loader_params)\n",
    "\n",
    "\n",
    "pd.Series({\n",
    "    'Number of batches in train DataLoader:': len(train_dataloader),\n",
    "    'Number of batches in validation DataLoader:': len(valid_dataloader)}\n",
    ").to_frame().style.hide(axis='columns')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Loading the Mask R-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 3\n",
    "\n",
    "# Initialize a Mask R-CNN model with pretrained weights\n",
    "model = maskrcnn_resnet50_fpn_v2(weights='DEFAULT')\n",
    "\n",
    "# Get the number of input features for the classifier\n",
    "in_features_box = model.roi_heads.box_predictor.cls_score.in_features\n",
    "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "\n",
    "# Get the numbner of output channels for the Mask Predictor\n",
    "dim_reduced = model.roi_heads.mask_predictor.conv5_mask.out_channels\n",
    "\n",
    "# Replace the box predictor\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_channels=in_features_box, num_classes=classes)\n",
    "\n",
    "# Replace the mask predictor\n",
    "model.roi_heads.mask_predictor = MaskRCNNPredictor(in_channels=in_features_mask, dim_reduced=dim_reduced, num_classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model's device and data type\n",
    "model.to(device=device, dtype=dtype);\n",
    "\n",
    "# Add attributes to store the device and model name for later reference\n",
    "model.device = device\n",
    "model.name = 'maskrcnn_resnet50_fpn_v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Summarize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kubaw\\miniforge3\\envs\\pytorch-env\\lib\\site-packages\\torchtnt\\utils\\module_summary.py:271: UserWarning: Backward FLOPs are only computed if module foward returns a tensor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th># Parameters</th>\n",
       "      <th># Trainable Parameters</th>\n",
       "      <th>Size (bytes)</th>\n",
       "      <th>Forward FLOPs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MaskRCNN</td>\n",
       "      <td>45.9 M</td>\n",
       "      <td>45.7 M</td>\n",
       "      <td>183 M</td>\n",
       "      <td>331 G</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Type # Parameters # Trainable Parameters Size (bytes) Forward FLOPs\n",
       "0  MaskRCNN       45.9 M                 45.7 M        183 M         331 G"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_summarize(model):\n",
    "\n",
    "    test_inp = torch.randn(1, 3, 256, 256).to(device)\n",
    "\n",
    "    summary_df = markdown_to_pandas(f\"{get_module_summary(model.eval(), [test_inp])}\")\n",
    "\n",
    "    # # Filter the summary to only contain Conv2d layers and the model\n",
    "    summary_df = summary_df[summary_df.index == 0]\n",
    "\n",
    "    return summary_df.drop(['In size', 'Out size', 'Contains Uninitialized Parameters?'], axis=1)\n",
    "\n",
    "model_summarize(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T07:46:27.157813Z",
     "iopub.status.busy": "2023-03-27T07:46:27.157375Z",
     "iopub.status.idle": "2023-03-27T07:46:27.164342Z",
     "shell.execute_reply": "2023-03-27T07:46:27.163373Z",
     "shell.execute_reply.started": "2023-03-27T07:46:27.157776Z"
    }
   },
   "source": [
    "## 8. Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T07:46:29.813497Z",
     "iopub.status.busy": "2023-03-27T07:46:29.813140Z",
     "iopub.status.idle": "2023-03-27T07:46:29.820564Z",
     "shell.execute_reply": "2023-03-27T07:46:29.819568Z",
     "shell.execute_reply.started": "2023-03-27T07:46:29.813467Z"
    }
   },
   "source": [
    "### 8.1 Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T07:46:29.960000Z",
     "iopub.status.busy": "2023-03-27T07:46:29.959690Z",
     "iopub.status.idle": "2023-03-27T07:46:29.974260Z",
     "shell.execute_reply": "2023-03-27T07:46:29.973392Z",
     "shell.execute_reply.started": "2023-03-27T07:46:29.959968Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# the following fx performs a single pass through the training set\n",
    "\n",
    "def run_epoch(model, dataloader, optimizer, lr_scheduler, device, scaler, is_training):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to run a single training or evaluation epoch.\n",
    "    \n",
    "    IN:\n",
    "        model: A PyTorch model to train or evaluate.\n",
    "        dataloader: A PyTorch DataLoader providing the data.\n",
    "        optimizer: The optimizer to use for training the model.\n",
    "        loss_func: The loss function used for training.\n",
    "        device: The device (CPU or GPU) to run the model on.\n",
    "        scaler: Gradient scaler for mixed-precision training.\n",
    "        is_training: Boolean flag indicating whether the model is in training or evaluation mode.\n",
    "    \n",
    "    Returns:\n",
    "        The average loss for the epoch.\n",
    "        \"\"\"\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0  # Initialize the total loss for this epoch\n",
    "    progress_bar = tqdm(total=len(dataloader), desc=\"Train\" if is_training else \"Eval\")  # Initialize a progress bar\n",
    "    \n",
    "    # Loop over the data\n",
    "    for batch_id, (inputs, targets) in enumerate(dataloader):\n",
    "        # Move inputs and targets to the specified device\n",
    "        inputs = torch.stack(inputs).to(device)\n",
    "        \n",
    "        # Forward pass with Automatic Mixed Precision (AMP) context manager\n",
    "        # provides convenience methods where some operations use the float32 datatype and other operations use float16\n",
    "        with autocast(torch.device(device).type):\n",
    "            if is_training:\n",
    "                losses = model(inputs.to(device), move_data_to_device(targets, device))\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    losses = model(inputs.to(device), move_data_to_device(targets, device))\n",
    "                    \n",
    "            # Compute the loss\n",
    "            loss = sum([loss for loss in losses.values()])  # Sum up the losses\n",
    "\n",
    "        # If in training mode, backpropagate the error and update the weights\n",
    "        if is_training:\n",
    "            if scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                old_scaler = scaler.get_scale()\n",
    "                scaler.update()\n",
    "                new_scaler = scaler.get_scale()\n",
    "                if new_scaler >= old_scaler:\n",
    "                    lr_scheduler.step()\n",
    "                    \n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        # Update the total loss\n",
    "        loss_item = loss.item()\n",
    "        epoch_loss += loss_item\n",
    "        \n",
    "        # Update the progress bar\n",
    "        progress_bar_dict = dict(loss=loss_item, avg_loss=epoch_loss/(batch_id+1))\n",
    "        if is_training:\n",
    "            progress_bar_dict.update(lr=lr_scheduler.get_last_lr()[0])\n",
    "        progress_bar.set_postfix(progress_bar_dict)\n",
    "        progress_bar.update()\n",
    "        \n",
    "        # If the loss is NaN or infinite, stop the training/evaluation process\n",
    "        if math.isnan(loss_item) or math.isinf(loss_item):\n",
    "            print(f\"Loss is NaN or infinite at batch {batch_id}. Stopping {'training' if is_training else 'evaluation'}.\")\n",
    "            break\n",
    "    # Cleanup and close the progress bar \n",
    "    progress_bar.close()\n",
    "    \n",
    "    # Return the average loss for this epoch\n",
    "    return epoch_loss / (batch_id + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T07:46:30.668035Z",
     "iopub.status.busy": "2023-03-27T07:46:30.667701Z",
     "iopub.status.idle": "2023-03-27T07:46:30.673224Z",
     "shell.execute_reply": "2023-03-27T07:46:30.672206Z",
     "shell.execute_reply.started": "2023-03-27T07:46:30.668006Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_loop(model, \n",
    "               train_dataloader, \n",
    "               valid_dataloader, \n",
    "               optimizer,  \n",
    "               lr_scheduler, \n",
    "               device, \n",
    "               epochs, \n",
    "               checkpoint_path, \n",
    "               use_scaler=False):\n",
    "    \"\"\"\n",
    "    Main training loop.\n",
    "    \n",
    "    Args:\n",
    "        model: A PyTorch model to train.\n",
    "        train_dataloader: A PyTorch DataLoader providing the training data.\n",
    "        valid_dataloader: A PyTorch DataLoader providing the validation data.\n",
    "        optimizer: The optimizer to use for training the model.\n",
    "        lr_scheduler: The learning rate scheduler.\n",
    "        device: The device (CPU or GPU) to run the model on.\n",
    "        epochs: The number of epochs to train for.\n",
    "        checkpoint_path: The path where to save the best model checkpoint.\n",
    "        use_scaler: Whether to scale graidents when using a CUDA device\n",
    "        \n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "    \n",
    "    # Initialize a gradient scaler for mixed-precision training if the device is a CUDA GPU\n",
    "    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' and use_scaler else None\n",
    "    best_loss = float('inf')  # Initialize the best validation loss\n",
    "    \n",
    "    \n",
    "    # Loop over the epochs\n",
    "    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "        # Run a training epoch and get the training loss\n",
    "        train_loss = run_epoch(model, train_dataloader, optimizer, lr_scheduler, device, scaler, is_training=True)\n",
    "        # Run an evaluation epoch and get the validation loss\n",
    "        with torch.no_grad():\n",
    "            valid_loss = run_epoch(model, valid_dataloader, None, None, device, scaler, is_training=False)\n",
    "\n",
    "\n",
    "                    # If the validation loss is lower than the best validation loss seen so far, save the model checkpoint\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "            # Save metadata about the training process\n",
    "            training_metadata = {\n",
    "                'epoch': epoch,\n",
    "                'train_loss': train_loss,\n",
    "                'valid_loss': valid_loss, \n",
    "                'learning_rate': lr_scheduler.get_last_lr()[0],\n",
    "                'model_architecture': model.name\n",
    "            }\n",
    "            with open(Path(checkpoint_path.parent/'training_metadata.json'), 'w') as f:\n",
    "                json.dump(training_metadata, f)\n",
    "                \n",
    "        # If the training or validation loss is NaN or infinite, stop the training process\n",
    "        if any(math.isnan(loss) or math.isinf(loss) for loss in [train_loss, valid_loss]):\n",
    "            print(f\"Loss is NaN or infinite at epoch {epoch}. Stopping training.\")\n",
    "            break\n",
    "\n",
    "    # If the device is a GPU, empty the cache\n",
    "    if device.type != 'cpu':\n",
    "        getattr(torch, device.type).empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T07:40:22.015025Z",
     "iopub.status.busy": "2023-03-27T07:40:22.012780Z",
     "iopub.status.idle": "2023-03-27T07:40:22.021288Z",
     "shell.execute_reply": "2023-03-27T07:40:22.020491Z",
     "shell.execute_reply.started": "2023-03-27T07:40:22.014972Z"
    }
   },
   "source": [
    "### 8.2 Set the Model Checkpoint Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T07:50:21.522384Z",
     "iopub.status.busy": "2023-03-27T07:50:21.522023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\pytorch-buildings-maskrcnn\\2023-10-14_15-20-25\\maskrcnn_resnet50_fpn_v2.pth\n"
     ]
    }
   ],
   "source": [
    "# Generate timestamp for the training session (Year-Month-Day_Hour_Minute_Second)\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Create a directory to store the checkpoints if it does not already exist\n",
    "checkpoint_dir = Path(project_dir/f\"{timestamp}\")\n",
    "\n",
    "# Create the checkpoint directory if it does not already exist\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# The model checkpoint path\n",
    "checkpoint_path = checkpoint_dir/f\"{model.name}.pth\"\n",
    "\n",
    "print(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T05:32:00.264708Z",
     "iopub.status.busy": "2023-03-26T05:32:00.264167Z",
     "iopub.status.idle": "2023-03-26T05:32:00.483405Z",
     "shell.execute_reply": "2023-03-26T05:32:00.482438Z",
     "shell.execute_reply.started": "2023-03-26T05:32:00.264657Z"
    }
   },
   "source": [
    "### 8.3 Configure the Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T05:32:03.705505Z",
     "iopub.status.busy": "2023-03-26T05:32:03.705112Z",
     "iopub.status.idle": "2023-03-26T05:32:03.882450Z",
     "shell.execute_reply": "2023-03-26T05:32:03.881409Z",
     "shell.execute_reply.started": "2023-03-26T05:32:03.705454Z"
    }
   },
   "outputs": [],
   "source": [
    "# Learning rate for the model\n",
    "lr = 5e-4\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 2\n",
    "\n",
    "# AdamW optimizer; includes weight decay for regularization\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# Learning rate scheduler; adjusts the learning rate during training\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                                   max_lr=lr, \n",
    "                                                   total_steps=epochs*len(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T05:32:13.441050Z",
     "iopub.status.busy": "2023-03-26T05:32:13.440353Z",
     "iopub.status.idle": "2023-03-26T05:32:16.446381Z",
     "shell.execute_reply": "2023-03-26T05:32:16.444816Z",
     "shell.execute_reply.started": "2023-03-26T05:32:13.440990Z"
    }
   },
   "source": [
    "## 9. Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T05:32:49.871701Z",
     "iopub.status.busy": "2023-03-26T05:32:49.871109Z",
     "iopub.status.idle": "2023-03-26T05:32:50.001893Z",
     "shell.execute_reply": "2023-03-26T05:32:50.000858Z",
     "shell.execute_reply.started": "2023-03-26T05:32:49.871656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f2ac7fa89e4131bb0662f3c405e7c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3fc89ddb56843d2b2ea2d612386285f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 14244, 17600, 6096, 10912, 10576, 8732, 19980, 17388) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniforge3\\envs\\pytorch-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1132\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\pytorch-env\\lib\\queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[146], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m           \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m           \u001b[49m\u001b[43mvalid_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m           \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m           \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m           \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m           \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m           \u001b[49m\u001b[43muse_scaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[141], line 36\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(model, train_dataloader, valid_dataloader, optimizer, lr_scheduler, device, epochs, checkpoint_path, use_scaler)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Loop over the epochs\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# Run a training epoch and get the training loss\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# Run an evaluation epoch and get the validation loss\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[1;32mIn[140], line 27\u001b[0m, in \u001b[0;36mrun_epoch\u001b[1;34m(model, dataloader, optimizer, lr_scheduler, device, scaler, is_training)\u001b[0m\n\u001b[0;32m     24\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_training \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEval\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Initialize a progress bar\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Loop over the data\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_id, (inputs, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Move inputs and targets to the specified device\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(inputs)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Forward pass with Automatic Mixed Precision (AMP) context manager\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# provides convenience methods where some operations use the float32 datatype and other operations use float16\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\pytorch-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\pytorch-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\pytorch-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1284\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1285\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1286\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\pytorch-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1145\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1144\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pids_str)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 14244, 17600, 6096, 10912, 10576, 8732, 19980, 17388) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "train_loop(model=model, \n",
    "           train_dataloader=train_dataloader,\n",
    "           valid_dataloader=valid_dataloader,\n",
    "           optimizer=optimizer, \n",
    "           lr_scheduler=lr_scheduler, \n",
    "           device=torch.device(device), \n",
    "           epochs=epochs, \n",
    "           checkpoint_path=checkpoint_path,\n",
    "           use_scaler=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T05:33:29.924066Z",
     "iopub.status.busy": "2023-03-26T05:33:29.923439Z",
     "iopub.status.idle": "2023-03-26T05:33:30.163566Z",
     "shell.execute_reply": "2023-03-26T05:33:30.162547Z",
     "shell.execute_reply.started": "2023-03-26T05:33:29.924027Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T05:33:17.896318Z",
     "iopub.status.busy": "2023-03-26T05:33:17.895952Z",
     "iopub.status.idle": "2023-03-26T05:33:17.902652Z",
     "shell.execute_reply": "2023-03-26T05:33:17.901724Z",
     "shell.execute_reply.started": "2023-03-26T05:33:17.896285Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
